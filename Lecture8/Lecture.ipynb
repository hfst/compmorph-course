{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da84d10c",
   "metadata": {},
   "source": [
    "# COMPUTATIONAL MORPHOLOGY WITH HFST TOOLS - LECTURE 8\n",
    "\n",
    "<ul>\n",
    " <li>1. <a href=\"#1.-Optimizing-unweighted-finite-state-networks\">Optimizing unweighted finite-state networks</a></li>\n",
    " <li>2. <a href=\"#2.-Optimizing-weighted-finite-state-networks\">Optimizing weighted finite-state networks</a></li>\n",
    "</ul>\n",
    "\n",
    "In this lecture we show how finite-state networks can be optimized,\n",
    "i.e. how the number of states and transitions can be made smaller.\n",
    "The networks that we use as examples will be created from scratch.\n",
    "So this lecture also shows how networks can be constructed state by state\n",
    "and transition by transition as opposed to more high-level ways such as\n",
    "regular expressions or lexc formalism.\n",
    "\n",
    "## 1. Optimizing unweighted finite-state networks\n",
    "\n",
    "### 1.1. Example lexicon\n",
    "\n",
    "Let’s first create a noun lexicon from scratch and add word stems to it.\n",
    "We use <code>HfstIterableTransducer</code> for this purpose.\n",
    "\n",
    "<img src=\"img/noun_lexicon.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45cee7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "!pip install hfst_dev\n",
    "from hfst_dev import HfstIterableTransducer, EPSILON\n",
    "# This will be the entire lexicon. The constructor creates a network\n",
    "# with one state, numbered as zero, which is an initial state.\n",
    "lexicon = HfstIterableTransducer()\n",
    "add_transition = lexicon.add_transition\n",
    "add_state = lexicon.add_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33601305",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define a function for adding a path between states state1 and state2:\n",
    "def add_path(path, state1, state2):\n",
    "    # This is done mainly to get the number for a new state.\n",
    "    # States don't need to be explicitly added, because 'add_transition'\n",
    "    # creates the start and target states of a given transition if they\n",
    "    # don't already exist.\n",
    "    state = add_state()\n",
    "    # Make sure state1 and state2 are skipped\n",
    "    if state == state1 or state == state2:\n",
    "        state += 1\n",
    "    if state == state1 or state == state2:\n",
    "        state += 1\n",
    "    add_transition(state1, state, EPSILON, EPSILON, 0.0) # from start state to beginning of path\n",
    "    for symbol in list(path):\n",
    "        add_transition(state, state+1, symbol, symbol, 0.0)\n",
    "        state += 1\n",
    "    add_transition(state, state2, EPSILON, EPSILON, 0.0) # from end of path to end state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control sublexicon start and end state numbering\n",
    "start_state = 1\n",
    "end_state = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the lexemes\n",
    "for lexeme in ('kisko', 'kissa','koira','kori','koulu','taulu','tori','tuoksu'):\n",
    "    add_path(lexeme, start_state, end_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a755f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the result is as intended:\n",
    "test_lexicon = HfstIterableTransducer(lexicon)\n",
    "test_lexicon.add_transition(0, 1, EPSILON, EPSILON, 0.0)\n",
    "test_lexicon.set_final_weight(8, 0.0)\n",
    "from hfst_dev import HfstTransducer, regex\n",
    "tr = HfstTransducer(test_lexicon)\n",
    "tr.minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = regex('{kisko}|{kissa}|{koira}|{kori}|{koulu}|{taulu}|{tori}|{tuoksu}')\n",
    "assert(result.compare(tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2423b5",
   "metadata": {},
   "source": [
    "Then let’s create a continuation lexicon with case endings and start populating it.\n",
    "\n",
    "<img src=\"img/continuation_lexicon.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f927e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state = add_state()\n",
    "assert(start_state == 50)\n",
    "end_state = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d767a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add case endings\n",
    "for ending in ('a','lla','lle','lta','n'):\n",
    "    add_path(ending, start_state, end_state)\n",
    "# make case ending optional\n",
    "add_transition(start_state, end_state, EPSILON, EPSILON, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a3fa2",
   "metadata": {},
   "source": [
    "Then we tie the lexicons together and also add an epsilon transition from the end of the stem lexicon to its beginning in order to allow compound words\n",
    "In many languages, this will mean: an epsilon transition from the end of the case ending lexicon at Sg.Nom/absolute.\n",
    "\n",
    "<img src=\"img/compound_lexicon.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_transition(8, start_state, EPSILON, EPSILON, 0.0)\n",
    "add_transition(8, 1, EPSILON, EPSILON, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d721b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the result is as intended\n",
    "test_lexicon = HfstIterableTransducer(lexicon)\n",
    "test_lexicon.add_transition(0, 1, EPSILON, EPSILON, 0.0)\n",
    "test_lexicon.set_final_weight(end_state, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstTransducer(test_lexicon)\n",
    "tr.minimize()\n",
    "result = regex('[{kisko}|{kissa}|{koira}|{kori}|{koulu}|{taulu}|{tori}|{tuoksu}]+ ({a}|{lla}|{lle}|{lta}|{n})')\n",
    "assert(result.compare(tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf25d7",
   "metadata": {},
   "source": [
    "Next let’s add a lexicon for verb stems.\n",
    "\n",
    "<img src=\"img/lexicon_verb_stems.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state = add_state()\n",
    "assert(start_state == 68)\n",
    "end_state = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82152459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add verb stem endings\n",
    "for stem in ('kisko','tuoksu'):\n",
    "    add_path(stem, start_state, end_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eef403",
   "metadata": {},
   "source": [
    "... and a continuation lexicon for present-tense person endings (mainly)\n",
    "\n",
    "<img src=\"img/lexicon_person_endings.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0848cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state = add_state()\n",
    "assert(start_state == 83)\n",
    "end_state = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add person endings\n",
    "for ending in ('a','mme','n','t','tte','vat'):\n",
    "    add_path(ending, start_state, end_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f63101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make person ending optional\n",
    "add_transition(start_state, end_state, EPSILON, EPSILON, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2eec6e",
   "metadata": {},
   "source": [
    "Let’s tie the verb stem lexicon together with the endings lexicon.\n",
    "\n",
    "<img src=\"img/lexicon_verbs_and_endings.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_transition(75, 83, EPSILON, EPSILON, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd94233",
   "metadata": {},
   "source": [
    "... and let’s tie the whole network together with a start state and end state\n",
    "\n",
    "<img src=\"img/lexicon_tied_together.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_transition(0, 1, EPSILON, EPSILON, 0.0)\n",
    "add_transition(0, 68, EPSILON, EPSILON, 0.0)\n",
    "add_transition(53, 103, EPSILON, EPSILON, 0.0)\n",
    "add_transition(86, 103, EPSILON, EPSILON, 0.0)\n",
    "lexicon.set_final_weight(103, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7809e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the result is as intended\n",
    "tr = HfstTransducer(lexicon)\n",
    "tr.minimize()\n",
    "result = regex(\"\"\"\n",
    "[ [{kisko}|{kissa}|{koira}|{kori}|{koulu}|{taulu}|{tori}|{tuoksu}]+ ({a}|{lla}|{lle}|{lta}|{n}) ]\n",
    "| \n",
    "[ [{kisko}|{tuoksu}] ({a}|{mme}|{n}|{t}|{tte}|{vat}) ]\n",
    "\"\"\")\n",
    "assert(result.compare(tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b97fa4",
   "metadata": {},
   "source": [
    "### 1.2. The resulting network\n",
    "\n",
    "The network is now ready. It has some advantages\n",
    "\n",
    "<ul>\n",
    " <li>The structure is logical</li>\n",
    " <li>Building the network by adding words and endings to it (through the union operation) is simple and fast</li>\n",
    "</ul>\n",
    "\n",
    "However, there are also some disadvantages\n",
    "\n",
    "<ul>\n",
    "<li>The automaton is non-deterministic and contains epsilon transitions</li>\n",
    " <ul>\n",
    "  <li>This means that from a specific state, some symbol s can take you to more than one other state.</li>\n",
    "  <li>For instance, from the initial state 0, the symbol “k” could take you to state 3, 10, 16, 22, 27, or 70.</li>\n",
    "  <li>Imagine a scenario with a more realistic, larger vocabulary: using the network would be very slow, because of all the paths that have to be investigated.</li>\n",
    " </ul>\n",
    "</ul>\n",
    "\n",
    "<img src=\"img/lexicon_epsilon_transitions.png\">\n",
    "\n",
    "### 1.3. Determinization of the network\n",
    "\n",
    "To start determinizing the network...\n",
    "\n",
    "<ul>\n",
    " <li>1. We would merge the states 3, 10, 16, 22, 27, and 70 into one single, new state.</li>\n",
    " <li>2. We would create one transition with the symbol “k” from the initial state to our new state. (Not shown in the picture on the next page.)</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"img/determinizing_the_network_1.png\">\n",
    "\n",
    "<ul>\n",
    " <li>3. Then, from the new state, the symbol “o” takes us to the states 17, 23 or 28, so we would merge these states into one new state, too.</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"img/determinizing_the_network_2.png\">\n",
    "\n",
    "<ul>\n",
    " <li>4. And the symbol “i” takes us to the states 4, 11, and 71, so we keep merging states and updating the transitions.</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"img/determinizing_the_network_3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b792af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstTransducer(lexicon)\n",
    "tr.determinize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36942dc0",
   "metadata": {},
   "source": [
    "### 1.4. Minimization of the network\n",
    "\n",
    "Furthermore, there is another disadvantage with the original network\n",
    "\n",
    "<ul>\n",
    "<li>The network is unnecessarily large</li>\n",
    " <ul>\n",
    "  <li>There are some “tails” that occur in many places that could be merged.</li>\n",
    "  <li>For instance, the ends of the stems “kori” and “tori” are identical, as are the ends of the stems “koulu” and “taulu”.</li>\n",
    "  <li>Determinization will not fix these issues, so we can use a separate minimization algorithm.</li>\n",
    " </ul>\n",
    "</ul>\n",
    "\n",
    "<img src=\"img/minimizing_the_network_1.png\">\n",
    "\n",
    "<img src=\"img/minimizing_the_network_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = HfstTransducer(lexicon)\n",
    "tr.minimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d30f8",
   "metadata": {},
   "source": [
    "### 1.5. Further issues\n",
    "\n",
    "<ul>\n",
    "<li>The epsilon transition back to the beginning that produces compound words is nasty:</li>\n",
    " <ul>\n",
    "  <li>Full determinization may actually bloat the size of the network.</li>\n",
    "  <li>Consider, for instance, if we had the stem “koulu”, which can take an “a” appended for partitive (“koulua”), but in addition, “a” could be the beginning of a second stem, such as “aamiainen” (“kouluaamiainen”).</li>\n",
    "  <li>Then, we would need one node in the network that is the starting point for all stems starting in “a” as the first stem in a word and another node with all the stems starting in “a” plus the endings starting in “a”.</li>\n",
    " </ul>\n",
    "</ul>\n",
    "\n",
    "<img src=\"img/full_determinization.png\">\n",
    "\n",
    "<ul>\n",
    " <li>One might actually choose not to do a full determinization, but keep the epsilon transitions, for instance.</li>\n",
    "</ul>\n",
    "\n",
    "#### Acceptors vs. transducers?\n",
    "\n",
    "<ul>\n",
    "<li>In the examples above, we have shown acceptors, with only an input symbol on the arcs</li>\n",
    " <ul>\n",
    "  <li>Such as: <code>a b c d</code></li>\n",
    " </ul>\n",
    "<li>If we had a transducer, we would have pairs of symbols (<code>input:output</code>)</li>\n",
    " <ul>\n",
    "  <li>Such as: <code>a:a a:b b:b c:e</code></li>\n",
    " </ul>\n",
    "<li>Determinization and minimization work the same in both cases.</li>\n",
    " <ul>\n",
    "  <li>We just need to interpret the pairs of symbols as one single symbol, so <code>a:a</code> is a different symbol from <code>a:b</code>.</li>\n",
    " </ul>\n",
    "</ul>\n",
    "\n",
    "#### Algorithms explained\n",
    "\n",
    "For instance, at www.tutorialspoint.com:\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"https://www.tutorialspoint.com/automata_theory/ndfa_to_dfa_conversion.htm\">determinization</a></li>\n",
    "<li><a href=\"https://www.tutorialspoint.com/automata_theory/dfa_minimization.htm\">minimization</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039175a4",
   "metadata": {},
   "source": [
    "## 2. Optimizing weighted finite-state networks\n",
    "\n",
    "Optimizing weighted finite-state networks is basically the same as unweighted networks, but the weights may mess things up.\n",
    "We would like the optimized weighted network to produce the same weights as the unoptimized network.\n",
    "\n",
    "Assume the following probabilities:\n",
    "\n",
    "```\n",
    "Prob(Noun) = 0.5\n",
    "Prob(Verb) = 0.3\n",
    "Prob(tuoksu | Noun) = 0.0001\n",
    "Prob(Noun ending -a for partitive case) = 0.1\n",
    "Prob(tuoksu | Verb) = 0.001\n",
    "Prob(Verb ending -a for infinitive) = 0.05\n",
    "```\n",
    "\n",
    "Then we get the following probabilities for the full word forms “tuoksua”:\n",
    "\n",
    "<ul>\n",
    " <li><code>Prob(tuoksua as a noun) = Prob(Noun) × Prob(tuoksu | Noun) × Prob(Noun ending -a for partitive case) = 0.5 × 0.0001 × 0.1 = 0.000005</code></li>\n",
    " <li><code>Prob(tuoksua as a verb) = Prob(Verb) × Prob(tuoksu | Verb) × Prob(Verb ending -a for infinitive) = 0.3 × 0.001 × 0.05 = 0.000015</code></li>\n",
    " <li><code>Prob(tuoksua as a noun or verb) = Prob(tuoksua as a noun) + Prob(tuoksua as a verb) =  0.000005 + 0.000015 = 0.00002</code></li>\n",
    "</ul>\n",
    "\n",
    "Shown as a network with probability weights:\n",
    "\n",
    "<img src=\"img/network_with_probability_weights.png\">\n",
    "\n",
    "However, usually weights are not probabilities as such.\n",
    "\n",
    "### 2.1. Semirings\n",
    "\n",
    "<ul>\n",
    "<li>Let’s replace the probabilities with some generic weights and replace the operators × and + with the generic semiring operators ⊗ and ⊕</li>\n",
    " <ul>\n",
    "  <li><code>Weight(tuoksua as a noun) = Weight(Noun) ⊗ Weight(tuoksu | Noun) ⊗ Weight(Noun ending -a for partitive case)</code></li>\n",
    "  <li><code>Weight(tuoksua as a verb) = Weight(Verb) ⊗ Weight(tuoksu | Verb) ⊗ Weight(Verb ending -a for infinitive)</code></li>\n",
    "  <li><code>Weight(tuoksua as a noun or verb) = Weight(tuoksua as a noun) ⊕ Weight(tuoksua as a verb)</code></li>\n",
    " </ul>\n",
    "</ul>\n",
    "\n",
    "### 2.2. Probability semiring\n",
    "\n",
    "<ul>\n",
    " <li>The weights should be interpreted as probabilties</li>\n",
    " <li>The operator ⊗ should be interpreted as multiplication ×</li>\n",
    " <li>The operator ⊕ should be interpreted as addition +</li>\n",
    " <li>This is exactly what we have seen in our example already</li>\n",
    "</ul>\n",
    "\n",
    "### 2.3. Log semiring\n",
    "\n",
    "<ul>\n",
    " <li>The weights should be interpreted as negative logprobs: for instance, – log Prob(tuoksu | Noun)</li>\n",
    " <li>The operator ⊗ should be interpreted as addition +</li>\n",
    " <li>The operator ⊕ should be interpreted as the rather complex operation: w1 ⊕ w2 = – log (10<sup>-w1</sup> + 10<sup>-w2</sup>)  (if we use 10 as our base; it can be something else, too)</li>\n",
    " <li>Why? Because if w1 = – log<sub>10</sub> p1 and w2 = – log<sub>10</sub> p2 and p1 and p2 are probabilities, then w1 ⊕ w2 = – log<sub>10</sub>(10<sup>– log<sub>10</sub> p1</sup> + 10<sup>– log<sub>10</sub> p2</sup>) =  – log<sub>10</sub>(p1 + p2) (This is the logprob of the sum of two probabilities)</li>\n",
    "</ul>\n",
    "\n",
    "Shown as a network with logprob weights in the log semiring\n",
    "\n",
    "<img src=\"img/network_with_logprob_weights.png\">\n",
    "\n",
    "### 2.4. Tropical semiring\n",
    "\n",
    "<ul>\n",
    " <li>The weights can still be interpreted as negative logprobs: for instance, –log(Prob(tuoksu | Noun))</li>\n",
    " <li>The operator ⊗ should still be interpreted as addition +</li>\n",
    " <li>The operator ⊕ is simplified and picks the minimum, that is, the path with lower overall weight: w1 ⊕ w2 = min(w1, w2) = { w1 if w1 < w2; w2 otherwise }</li>\n",
    "</ul>\n",
    "\n",
    "Shown as a network with logprob weights in the tropical semiring:\n",
    "\n",
    "<img src=\"img/network_with_tropical_weights.png\">\n",
    "\n",
    "#### Another example of weighted determinization in the tropical semiring\n",
    "\n",
    "<img src=\"img/weighted_determinization_example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ff12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network from ATT format:\n",
    "from hfst_dev import read_att_string\n",
    "tr = read_att_string(\n",
    "\"\"\"0 1 a a 0\n",
    "0 1 b b 1\n",
    "0 1 c c 4\n",
    "0 2 a a 3\n",
    "0 2 b b 4\n",
    "0 2 c c 7\n",
    "0 2 d d 0\n",
    "0 2 e e 1\n",
    "1 3 f f 1\n",
    "1 3 e e 0\n",
    "1 3 e e 2\n",
    "2 3 e e 10\n",
    "2 3 f f 11\n",
    "2 3 f f 13\n",
    "3 0\n",
    "\"\"\")\n",
    "tr.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a735ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.determinize()\n",
    "tr.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0fdb1",
   "metadata": {},
   "source": [
    "Weights must be pushed before minimization can take place:\n",
    "\n",
    "<img src=\"img/weight_pushing_and_minimization.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.minimize()\n",
    "tr.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0b25a",
   "metadata": {},
   "source": [
    "#### Other uses\n",
    "\n",
    "Mehryar Mohri did not work on morphology, but on automatic speech recognition:\n",
    "\n",
    "<img src=\"img/speech_recognition.png\">\n",
    "\n",
    "## Further reading\n",
    "\n",
    "<ul>\n",
    " <li>To learn more, you can read the <a href=\"http://www.cs.nyu.edu/~mohri/pub/csl01.pdf\">full article</a> by Mohri et al.</li>\n",
    " <li>There are more similar articles, such as the version that was actually published in Computer Speech and Language in 2002.</li>\n",
    " <li>Or look at the <a href=\"http://www.openfst.org\">OpenFST library</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
